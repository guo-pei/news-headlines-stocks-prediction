{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "pd.options.display.max_rows = 6000\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[georgia, two, russian, warplane, country, mov...</td>\n",
       "      <td>[-0.0046601472, 0.046057668, 0.035470575, 0.10...</td>\n",
       "      <td>georgia two russian warplane country move brin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[wont, america, nato, help, wont, help, help, ...</td>\n",
       "      <td>[-0.01796527, 0.026893076, 0.05216946, 0.11043...</td>\n",
       "      <td>wont america nato help wont help help iraq put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[adorable, sang, opening, ceremony, wa, fake, ...</td>\n",
       "      <td>[0.020226372, 0.05665661, 0.038335405, 0.09110...</td>\n",
       "      <td>adorable sang opening ceremony wa fake russia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[america, refuse, israel, weapon, attack, iran...</td>\n",
       "      <td>[0.009319111, 0.04263116, 0.062353328, 0.08478...</td>\n",
       "      <td>america refuse israel weapon attack iran repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[expert, admit, legalise, drug, south, osetia,...</td>\n",
       "      <td>[0.01713654, 0.04969087, 0.062367942, 0.105228...</td>\n",
       "      <td>expert admit legalise drug south osetia pictur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [georgia, two, russian, warplane, country, mov...   \n",
       "1      1  [wont, america, nato, help, wont, help, help, ...   \n",
       "2      0  [adorable, sang, opening, ceremony, wa, fake, ...   \n",
       "3      0  [america, refuse, israel, weapon, attack, iran...   \n",
       "4      1  [expert, admit, legalise, drug, south, osetia,...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.0046601472, 0.046057668, 0.035470575, 0.10...   \n",
       "1  [-0.01796527, 0.026893076, 0.05216946, 0.11043...   \n",
       "2  [0.020226372, 0.05665661, 0.038335405, 0.09110...   \n",
       "3  [0.009319111, 0.04263116, 0.062353328, 0.08478...   \n",
       "4  [0.01713654, 0.04969087, 0.062367942, 0.105228...   \n",
       "\n",
       "                                            news_str  \n",
       "0  georgia two russian warplane country move brin...  \n",
       "1  wont america nato help wont help help iraq put...  \n",
       "2  adorable sang opening ceremony wa fake russia ...  \n",
       "3  america refuse israel weapon attack iran repor...  \n",
       "4  expert admit legalise drug south osetia pictur...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('label_news_docvec_newsstr.pkl', 'rb') as r:\n",
    "    df = pickle.load(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling to vectors  (unsupervised learning)\n",
    "\n",
    "data allocation:<br>\n",
    "1. for topic modeling: 15% data set aside for testing. use the 85% for topic modeling. \n",
    "2. apply the topic model to the testing data to get the topic vectors.\n",
    "3. create the final train, valid, test files for AWS\n",
    "\n",
    "topic modeling models:<br>\n",
    "1. use HDP to decide the topic size\n",
    "2. use LDA to determine the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688 298\n"
     ]
    }
   ],
   "source": [
    "df_HDP_train = df.news[:1688]\n",
    "df_LDA_train = df.news_str[:1688]   # 85% of the news_str data is used to train topic model\n",
    "df_LDA_test = df.news_str[1688:]\n",
    "print(len(df_LDA_train), len(df_LDA_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP corpus and dictionary (need to be bag of words format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_hdp = gensim.corpora.Dictionary(df_HDP_train)\n",
    "id2word_hdp.filter_extremes(no_below=10, no_above=0.30)\n",
    "id2word_hdp.compactify()\n",
    "id2word_hdp.save('train_dict_hdp')\n",
    "corpus_hdp = [id2word_hdp.doc2bow(doc) for doc in df_HDP_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use HDP model to decide the maxium topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "hdp = HdpModel(corpus_hdp, id2word_hdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hdp.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*amp + 0.003*ukraine + 0.003*syria + 0.002*isis + 0.002*oil + 0.002*japan + 0.002*drug + 0.002*city + 0.002*germany + 0.002*power'),\n",
       " (1,\n",
       "  '0.003*wikileaks + 0.002*city + 0.002*syria + 0.002*drug + 0.002*egypt + 0.002*strike + 0.002*amp + 0.002*internet + 0.002*pakistan + 0.002*take'),\n",
       " (2,\n",
       "  '0.002*oil + 0.001*japan + 0.001*get + 0.001*gaza + 0.001*libya + 0.001*killing + 0.001*saudi + 0.001*claim + 0.001*billion + 0.001*back'),\n",
       " (3,\n",
       "  '0.001*video + 0.001*libyan + 0.001*egypt + 0.001*bin + 0.001*libya + 0.001*arrested + 0.001*laden + 0.001*election + 0.001*protester + 0.001*afghanistan'),\n",
       " (4,\n",
       "  '0.002*invest + 0.001*wikileaks + 0.001*mayor + 0.001*plan + 0.001*trick + 0.001*famine + 0.001*fire + 0.001*church + 0.001*iraq + 0.001*get'),\n",
       " (5,\n",
       "  '0.002*gaza + 0.002*wikileaks + 0.002*video + 0.001*iranian + 0.001*system + 0.001*killing + 0.001*east + 0.001*kill + 0.001*australia + 0.001*music'),\n",
       " (6,\n",
       "  '0.001*soldier + 0.001*palestinian + 0.001*wikileaks + 0.001*canada + 0.001*afghanistan + 0.001*election + 0.001*kosovo + 0.001*gaza + 0.001*trafficker + 0.001*city'),\n",
       " (7,\n",
       "  '0.002*gaza + 0.001*bite + 0.001*benjamin + 0.001*played + 0.001*foreign + 0.001*distance + 0.001*pope + 0.001*replaced + 0.001*prosecutor + 0.001*became'),\n",
       " (8,\n",
       "  '0.002*greece + 0.001*german + 0.001*telescope + 0.001*defense + 0.001*outlawed + 0.001*dealing + 0.001*video + 0.001*inspired + 0.001*internet + 0.001*al'),\n",
       " (9,\n",
       "  '0.001*honduras + 0.001*warm + 0.001*unconstitutional + 0.001*send + 0.001*activity + 0.001*moldova + 0.001*legalizes + 0.001*way + 0.001*georgia + 0.001*serve'),\n",
       " (10,\n",
       "  '0.001*investigative + 0.001*switzerland + 0.001*repressive + 0.001*kill + 0.001*tracked + 0.001*played + 0.001*see + 0.001*wikileaks + 0.001*upcoming + 0.001*reason'),\n",
       " (11,\n",
       "  '0.002*dollar + 0.001*chavez + 0.001*brian + 0.001*targeted + 0.001*union + 0.001*muslims + 0.001*sponsor + 0.001*requiring + 0.001*inspired + 0.001*pope'),\n",
       " (12,\n",
       "  '0.001*married + 0.001*iraq + 0.001*okinawa + 0.001*meal + 0.001*pakistan + 0.001*viewer + 0.001*bomb + 0.001*japan + 0.001*gaza + 0.001*requiring'),\n",
       " (13,\n",
       "  '0.001*deploy + 0.001*dog + 0.001*introduce + 0.001*particularly + 0.001*irish + 0.001*ecosystem + 0.001*blaming + 0.001*info + 0.001*laying + 0.001*looked'),\n",
       " (14,\n",
       "  '0.001*launching + 0.001*eventually + 0.001*watchdog + 0.001*falling + 0.001*favor + 0.001*turkey + 0.001*hormuz + 0.001*system + 0.001*writes + 0.001*stronghold'),\n",
       " (15,\n",
       "  '0.002*image + 0.001*pakistan + 0.001*taser + 0.001*christians + 0.001*written + 0.001*unless + 0.001*pilgrimage + 0.001*print + 0.001*agriculture + 0.001*confirms'),\n",
       " (16,\n",
       "  '0.001*extremists + 0.001*identity + 0.001*fly + 0.001*surrender + 0.001*plant + 0.001*deutsche + 0.001*japan + 0.001*palm + 0.001*bomb + 0.001*spends'),\n",
       " (17,\n",
       "  '0.001*asked + 0.001*jews + 0.001*garment + 0.001*bullied + 0.001*eastern + 0.001*everyday + 0.001*victims + 0.001*taking + 0.001*swim + 0.001*real'),\n",
       " (18,\n",
       "  '0.001*jewish + 0.001*richest + 0.001*survival + 0.001*suing + 0.001*things + 0.001*venezuela + 0.001*afghan + 0.001*selling + 0.001*power + 0.001*snap'),\n",
       " (19,\n",
       "  '0.001*offensive + 0.001*crucial + 0.001*nbc + 0.001*overnight + 0.001*jail + 0.001*painted + 0.001*barely + 0.001*bought + 0.001*introduced + 0.001*good')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.print_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modeling for topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to have more stopwords for topic modeling than for word2vec.\n",
    "# NLTK + SKlearn + self definded\n",
    "sk_stop = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "mywords = ['whilst', 'say', 'says', 'today','yesterday', 'news', 'tomorrow','iii', 'ii', 'like', 'ha','wa']\n",
    "final_stop = stopwords.words('english') + mywords + sk_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(stop_words = final_stop, ngram_range = (1, 2), max_df = 0.95)\n",
    "doc_word = tfv.fit_transform(df_LDA_train).transpose()\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "id2word = dict((v, k) for k, v in tfv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=5, random_state = 200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"new\" + 0.000*\"war\" + 0.000*\"israel\" + 0.000*\"police\" + 0.000*\"people\" + 0.000*\"year\" + 0.000*\"russia\" + 0.000*\"world\" + 0.000*\"government\" + 0.000*\"attack\"'),\n",
       " (1,\n",
       "  '0.000*\"israeli\" + 0.000*\"israel\" + 0.000*\"mumbai\" + 0.000*\"russia\" + 0.000*\"war\" + 0.000*\"china\" + 0.000*\"new\" + 0.000*\"year\" + 0.000*\"police\" + 0.000*\"fukushima\"'),\n",
       " (2,\n",
       "  '0.000*\"korea\" + 0.000*\"israel\" + 0.000*\"china\" + 0.000*\"libya\" + 0.000*\"north\" + 0.000*\"government\" + 0.000*\"new\" + 0.000*\"year\" + 0.000*\"world\" + 0.000*\"killed\"'),\n",
       " (3,\n",
       "  '0.000*\"russia\" + 0.000*\"china\" + 0.000*\"government\" + 0.000*\"new\" + 0.000*\"iran\" + 0.000*\"world\" + 0.000*\"people\" + 0.000*\"america\" + 0.000*\"year\" + 0.000*\"russian\"'),\n",
       " (4,\n",
       "  '0.001*\"israel\" + 0.001*\"new\" + 0.001*\"world\" + 0.001*\"year\" + 0.001*\"government\" + 0.001*\"china\" + 0.001*\"police\" + 0.001*\"russia\" + 0.001*\"people\" + 0.001*\"war\"'),\n",
       " (5,\n",
       "  '0.000*\"israel\" + 0.000*\"new\" + 0.000*\"ukraine\" + 0.000*\"world\" + 0.000*\"isis\" + 0.000*\"gaza\" + 0.000*\"russia\" + 0.000*\"state\" + 0.000*\"police\" + 0.000*\"china\"'),\n",
       " (6,\n",
       "  '0.000*\"year\" + 0.000*\"israel\" + 0.000*\"china\" + 0.000*\"police\" + 0.000*\"new\" + 0.000*\"world\" + 0.000*\"gaza\" + 0.000*\"child\" + 0.000*\"war\" + 0.000*\"north\"'),\n",
       " (7,\n",
       "  '0.000*\"israel\" + 0.000*\"china\" + 0.000*\"syria\" + 0.000*\"killed\" + 0.000*\"government\" + 0.000*\"police\" + 0.000*\"new\" + 0.000*\"russia\" + 0.000*\"year\" + 0.000*\"world\"'),\n",
       " (8,\n",
       "  '0.000*\"libya\" + 0.000*\"people\" + 0.000*\"war\" + 0.000*\"country\" + 0.000*\"police\" + 0.000*\"north\" + 0.000*\"world\" + 0.000*\"china\" + 0.000*\"israel\" + 0.000*\"korea\"'),\n",
       " (9,\n",
       "  '0.000*\"china\" + 0.000*\"world\" + 0.000*\"government\" + 0.000*\"year\" + 0.000*\"new\" + 0.000*\"woman\" + 0.000*\"war\" + 0.000*\"amp\" + 0.000*\"israel\" + 0.000*\"america\"'),\n",
       " (10,\n",
       "  '0.000*\"police\" + 0.000*\"new\" + 0.000*\"america\" + 0.000*\"year\" + 0.000*\"government\" + 0.000*\"world\" + 0.000*\"embassy\" + 0.000*\"war\" + 0.000*\"attack\" + 0.000*\"president\"'),\n",
       " (11,\n",
       "  '0.000*\"china\" + 0.000*\"new\" + 0.000*\"people\" + 0.000*\"year\" + 0.000*\"uk\" + 0.000*\"government\" + 0.000*\"israel\" + 0.000*\"police\" + 0.000*\"country\" + 0.000*\"killed\"'),\n",
       " (12,\n",
       "  '0.000*\"georgia\" + 0.000*\"south ossetia\" + 0.000*\"ossetia\" + 0.000*\"israel\" + 0.000*\"iran\" + 0.000*\"russia\" + 0.000*\"south\" + 0.000*\"america\" + 0.000*\"russian\" + 0.000*\"war\"'),\n",
       " (13,\n",
       "  '0.000*\"israel\" + 0.000*\"korea\" + 0.000*\"north\" + 0.000*\"police\" + 0.000*\"russia\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"people\" + 0.000*\"child\" + 0.000*\"year\"'),\n",
       " (14,\n",
       "  '0.000*\"world\" + 0.000*\"china\" + 0.000*\"russia\" + 0.000*\"israel\" + 0.000*\"year\" + 0.000*\"police\" + 0.000*\"america\" + 0.000*\"new\" + 0.000*\"people\" + 0.000*\"iran\"'),\n",
       " (15,\n",
       "  '0.000*\"new\" + 0.000*\"government\" + 0.000*\"world\" + 0.000*\"israel\" + 0.000*\"china\" + 0.000*\"year\" + 0.000*\"israeli\" + 0.000*\"country\" + 0.000*\"president\" + 0.000*\"people\"'),\n",
       " (16,\n",
       "  '0.000*\"israel\" + 0.000*\"government\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"iran\" + 0.000*\"police\" + 0.000*\"year\" + 0.000*\"china\" + 0.000*\"russia\" + 0.000*\"protester\"'),\n",
       " (17,\n",
       "  '0.000*\"police\" + 0.000*\"government\" + 0.000*\"israeli\" + 0.000*\"israel\" + 0.000*\"iran\" + 0.000*\"new\" + 0.000*\"america\" + 0.000*\"china\" + 0.000*\"country\" + 0.000*\"nuclear\"'),\n",
       " (18,\n",
       "  '0.000*\"israel\" + 0.000*\"world\" + 0.000*\"russia\" + 0.000*\"year\" + 0.000*\"country\" + 0.000*\"syria\" + 0.000*\"new\" + 0.000*\"israeli\" + 0.000*\"war\" + 0.000*\"amp\"'),\n",
       " (19,\n",
       "  '0.000*\"russia\" + 0.000*\"new\" + 0.000*\"japan\" + 0.000*\"nuclear\" + 0.000*\"military\" + 0.000*\"year\" + 0.000*\"government\" + 0.000*\"russian\" + 0.000*\"world\" + 0.000*\"korea\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make topic vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1688)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(df_LDA_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1688"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.7669369,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.012266482,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648,\n",
       " 0.01226648]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vecs_train = []\n",
    "for i in range(len(df_LDA_train)):\n",
    "    doc_topics = lda.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    doc_top_vec = [doc_topics[num][1] for num in range(20)]\n",
    "    top_vecs_train.append(doc_top_vec)\n",
    "\n",
    "print(len(top_vecs_train))\n",
    "top_vecs_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.23040833,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828,\n",
       " 0.040504828]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the topic model from train data to get get data topic vectors\n",
    "doc_word_test = tfv.fit_transform(df_LDA_test).transpose()\n",
    "corpus_test = matutils.Sparse2Corpus(doc_word_test)\n",
    "\n",
    "top_vecs_test = []\n",
    "for i in range(len(df_LDA_test)):\n",
    "    doc_topics_test = lda.get_document_topics(corpus_test[i], minimum_probability=0.0)\n",
    "    doc_top_vec_test = [doc_topics_test[num][1] for num in range(20)]\n",
    "    top_vecs_test.append(doc_top_vec_test)\n",
    "\n",
    "print(len(top_vecs_test))\n",
    "top_vecs_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two lists then convert to a Seires adding to the full dataframe for train, valid, test split.\n",
    "top_vecs = pd.Series(top_vecs_train + top_vecs_test, name = 'top_vecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "      <th>top_vecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[georgia, two, russian, warplane, country, mov...</td>\n",
       "      <td>[-0.0046601472, 0.046057668, 0.035470575, 0.10...</td>\n",
       "      <td>georgia two russian warplane country move brin...</td>\n",
       "      <td>[0.012460786, 0.012460786, 0.012460786, 0.0124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[wont, america, nato, help, wont, help, help, ...</td>\n",
       "      <td>[-0.01796527, 0.026893076, 0.05216946, 0.11043...</td>\n",
       "      <td>wont america nato help wont help help iraq put...</td>\n",
       "      <td>[0.01266635, 0.01266635, 0.01266635, 0.0126663...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[adorable, sang, opening, ceremony, wa, fake, ...</td>\n",
       "      <td>[0.020226372, 0.05665661, 0.038335405, 0.09110...</td>\n",
       "      <td>adorable sang opening ceremony wa fake russia ...</td>\n",
       "      <td>[0.013141308, 0.013141308, 0.013141308, 0.0131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[america, refuse, israel, weapon, attack, iran...</td>\n",
       "      <td>[0.009319111, 0.04263116, 0.062353328, 0.08478...</td>\n",
       "      <td>america refuse israel weapon attack iran repor...</td>\n",
       "      <td>[0.0119563425, 0.0119563425, 0.0119563425, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[expert, admit, legalise, drug, south, osetia,...</td>\n",
       "      <td>[0.01713654, 0.04969087, 0.062367942, 0.105228...</td>\n",
       "      <td>expert admit legalise drug south osetia pictur...</td>\n",
       "      <td>[0.012844525, 0.012844525, 0.012844525, 0.0128...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [georgia, two, russian, warplane, country, mov...   \n",
       "1      1  [wont, america, nato, help, wont, help, help, ...   \n",
       "2      0  [adorable, sang, opening, ceremony, wa, fake, ...   \n",
       "3      0  [america, refuse, israel, weapon, attack, iran...   \n",
       "4      1  [expert, admit, legalise, drug, south, osetia,...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.0046601472, 0.046057668, 0.035470575, 0.10...   \n",
       "1  [-0.01796527, 0.026893076, 0.05216946, 0.11043...   \n",
       "2  [0.020226372, 0.05665661, 0.038335405, 0.09110...   \n",
       "3  [0.009319111, 0.04263116, 0.062353328, 0.08478...   \n",
       "4  [0.01713654, 0.04969087, 0.062367942, 0.105228...   \n",
       "\n",
       "                                            news_str  \\\n",
       "0  georgia two russian warplane country move brin...   \n",
       "1  wont america nato help wont help help iraq put...   \n",
       "2  adorable sang opening ceremony wa fake russia ...   \n",
       "3  america refuse israel weapon attack iran repor...   \n",
       "4  expert admit legalise drug south osetia pictur...   \n",
       "\n",
       "                                            top_vecs  \n",
       "0  [0.012460786, 0.012460786, 0.012460786, 0.0124...  \n",
       "1  [0.01266635, 0.01266635, 0.01266635, 0.0126663...  \n",
       "2  [0.013141308, 0.013141308, 0.013141308, 0.0131...  \n",
       "3  [0.0119563425, 0.0119563425, 0.0119563425, 0.0...  \n",
       "4  [0.012844525, 0.012844525, 0.012844525, 0.0128...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['top_vecs'] = top_vecs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "1986\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "      <th>top_vecs</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[georgia, two, russian, warplane, country, mov...</td>\n",
       "      <td>[-0.0046601472, 0.046057668, 0.035470575, 0.10...</td>\n",
       "      <td>georgia two russian warplane country move brin...</td>\n",
       "      <td>[0.012460786, 0.012460786, 0.012460786, 0.0124...</td>\n",
       "      <td>[-0.0046601472, 0.046057668, 0.035470575, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[wont, america, nato, help, wont, help, help, ...</td>\n",
       "      <td>[-0.01796527, 0.026893076, 0.05216946, 0.11043...</td>\n",
       "      <td>wont america nato help wont help help iraq put...</td>\n",
       "      <td>[0.01266635, 0.01266635, 0.01266635, 0.0126663...</td>\n",
       "      <td>[-0.01796527, 0.026893076, 0.05216946, 0.11043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[adorable, sang, opening, ceremony, wa, fake, ...</td>\n",
       "      <td>[0.020226372, 0.05665661, 0.038335405, 0.09110...</td>\n",
       "      <td>adorable sang opening ceremony wa fake russia ...</td>\n",
       "      <td>[0.013141308, 0.013141308, 0.013141308, 0.0131...</td>\n",
       "      <td>[0.020226372, 0.05665661, 0.038335405, 0.09110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[america, refuse, israel, weapon, attack, iran...</td>\n",
       "      <td>[0.009319111, 0.04263116, 0.062353328, 0.08478...</td>\n",
       "      <td>america refuse israel weapon attack iran repor...</td>\n",
       "      <td>[0.0119563425, 0.0119563425, 0.0119563425, 0.0...</td>\n",
       "      <td>[0.009319111, 0.04263116, 0.062353328, 0.08478...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[expert, admit, legalise, drug, south, osetia,...</td>\n",
       "      <td>[0.01713654, 0.04969087, 0.062367942, 0.105228...</td>\n",
       "      <td>expert admit legalise drug south osetia pictur...</td>\n",
       "      <td>[0.012844525, 0.012844525, 0.012844525, 0.0128...</td>\n",
       "      <td>[0.01713654, 0.04969087, 0.062367942, 0.105228...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [georgia, two, russian, warplane, country, mov...   \n",
       "1      1  [wont, america, nato, help, wont, help, help, ...   \n",
       "2      0  [adorable, sang, opening, ceremony, wa, fake, ...   \n",
       "3      0  [america, refuse, israel, weapon, attack, iran...   \n",
       "4      1  [expert, admit, legalise, drug, south, osetia,...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.0046601472, 0.046057668, 0.035470575, 0.10...   \n",
       "1  [-0.01796527, 0.026893076, 0.05216946, 0.11043...   \n",
       "2  [0.020226372, 0.05665661, 0.038335405, 0.09110...   \n",
       "3  [0.009319111, 0.04263116, 0.062353328, 0.08478...   \n",
       "4  [0.01713654, 0.04969087, 0.062367942, 0.105228...   \n",
       "\n",
       "                                            news_str  \\\n",
       "0  georgia two russian warplane country move brin...   \n",
       "1  wont america nato help wont help help iraq put...   \n",
       "2  adorable sang opening ceremony wa fake russia ...   \n",
       "3  america refuse israel weapon attack iran repor...   \n",
       "4  expert admit legalise drug south osetia pictur...   \n",
       "\n",
       "                                            top_vecs  \\\n",
       "0  [0.012460786, 0.012460786, 0.012460786, 0.0124...   \n",
       "1  [0.01266635, 0.01266635, 0.01266635, 0.0126663...   \n",
       "2  [0.013141308, 0.013141308, 0.013141308, 0.0131...   \n",
       "3  [0.0119563425, 0.0119563425, 0.0119563425, 0.0...   \n",
       "4  [0.012844525, 0.012844525, 0.012844525, 0.0128...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [-0.0046601472, 0.046057668, 0.035470575, 0.10...  \n",
       "1  [-0.01796527, 0.026893076, 0.05216946, 0.11043...  \n",
       "2  [0.020226372, 0.05665661, 0.038335405, 0.09110...  \n",
       "3  [0.009319111, 0.04263116, 0.062353328, 0.08478...  \n",
       "4  [0.01713654, 0.04969087, 0.062367942, 0.105228...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two vector columns for later modeling\n",
    "df['vectors'] = pd.Series([list(df.doc_vec[row]) + list(df.top_vecs[row]) for row in range(len(df))])\n",
    "\n",
    "print(len(df.vectors[0]))\n",
    "print(len(df.vectors))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_final_6col.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_final_6col.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## from the results below, our Naive Bayes base line model is still the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  svm, naive_bayes, neighbors, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "nb_model = naive_bayes.GaussianNB()\n",
    "knn_model = neighbors.KNeighborsClassifier()\n",
    "svc_model = svm.SVC(probability=True, gamma=\"scale\")\n",
    "rf_model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "et_model = ensemble.ExtraTreesClassifier(n_estimators=100)\n",
    "ada_model = ensemble.AdaBoostClassifier()\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, \n",
    "                              reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n",
    "\n",
    "models = [\"lr_model\", \"nb_model\", \"knn_model\", \"svc_model\", \"rf_model\", \"et_model\", \"ada_model\", \"xgb_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_filter(modellist, X, y):\n",
    "    ''' 1. split the train data further into train and validation (17%). \n",
    "        2. fit the train data into each model of the model list\n",
    "        3. get the classification report based on the model performance on validation data\n",
    "    '''\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.17, random_state = 100)\n",
    "    for model_name in modellist:\n",
    "        curr_model = eval(model_name)\n",
    "        curr_model.fit(X_train, y_train) \n",
    "        print(f'{model_name} \\n report:{classification_report(y_valid, curr_model.predict(X_valid))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec + topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.02      0.05       122\n",
      "           1       0.57      0.97      0.72       165\n",
      "\n",
      "    accuracy                           0.57       287\n",
      "   macro avg       0.47      0.50      0.38       287\n",
      "weighted avg       0.49      0.57      0.43       287\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.37      0.38       122\n",
      "           1       0.56      0.59      0.58       165\n",
      "\n",
      "    accuracy                           0.50       287\n",
      "   macro avg       0.48      0.48      0.48       287\n",
      "weighted avg       0.49      0.50      0.49       287\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.36      0.38       122\n",
      "           1       0.57      0.62      0.59       165\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.49      0.49      0.49       287\n",
      "weighted avg       0.50      0.51      0.50       287\n",
      "\n",
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       122\n",
      "           1       0.57      1.00      0.73       165\n",
      "\n",
      "    accuracy                           0.57       287\n",
      "   macro avg       0.29      0.50      0.37       287\n",
      "weighted avg       0.33      0.57      0.42       287\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.30      0.35       122\n",
      "           1       0.57      0.68      0.62       165\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.49      0.49      0.48       287\n",
      "weighted avg       0.50      0.52      0.50       287\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.29      0.32       122\n",
      "           1       0.55      0.64      0.59       165\n",
      "\n",
      "    accuracy                           0.49       287\n",
      "   macro avg       0.46      0.46      0.46       287\n",
      "weighted avg       0.47      0.49      0.48       287\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.44      0.44       122\n",
      "           1       0.58      0.56      0.57       165\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.50      0.50      0.50       287\n",
      "weighted avg       0.51      0.51      0.51       287\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.42      0.44       122\n",
      "           1       0.60      0.64      0.62       165\n",
      "\n",
      "    accuracy                           0.55       287\n",
      "   macro avg       0.53      0.53      0.53       287\n",
      "weighted avg       0.54      0.55      0.54       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(df.vectors[:1688]))\n",
    "y = np.array(list(df.Label[:1688]))\n",
    "\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       122\n",
      "           1       0.57      1.00      0.73       165\n",
      "\n",
      "    accuracy                           0.57       287\n",
      "   macro avg       0.29      0.50      0.37       287\n",
      "weighted avg       0.33      0.57      0.42       287\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.19      0.25       122\n",
      "           1       0.55      0.75      0.64       165\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.45      0.47      0.44       287\n",
      "weighted avg       0.47      0.51      0.47       287\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.41      0.43       122\n",
      "           1       0.59      0.64      0.61       165\n",
      "\n",
      "    accuracy                           0.54       287\n",
      "   macro avg       0.52      0.52      0.52       287\n",
      "weighted avg       0.53      0.54      0.54       287\n",
      "\n",
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       122\n",
      "           1       0.57      1.00      0.73       165\n",
      "\n",
      "    accuracy                           0.57       287\n",
      "   macro avg       0.29      0.50      0.37       287\n",
      "weighted avg       0.33      0.57      0.42       287\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.52      0.46       122\n",
      "           1       0.57      0.48      0.52       165\n",
      "\n",
      "    accuracy                           0.49       287\n",
      "   macro avg       0.50      0.50      0.49       287\n",
      "weighted avg       0.51      0.49      0.50       287\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.49      0.45       122\n",
      "           1       0.57      0.50      0.53       165\n",
      "\n",
      "    accuracy                           0.49       287\n",
      "   macro avg       0.49      0.49      0.49       287\n",
      "weighted avg       0.51      0.49      0.50       287\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       122\n",
      "           1       0.55      0.56      0.56       165\n",
      "\n",
      "    accuracy                           0.49       287\n",
      "   macro avg       0.47      0.47      0.47       287\n",
      "weighted avg       0.49      0.49      0.49       287\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.30      0.34       122\n",
      "           1       0.56      0.65      0.60       165\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.48      0.48      0.47       287\n",
      "weighted avg       0.49      0.51      0.49       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(df.top_vecs[:1688]))\n",
    "y = np.array(list(df.Label[:1688]))\n",
    "\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
